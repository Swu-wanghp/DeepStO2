# # model.py
# import math
# import numpy as np
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
#
#
# # -----------------------------
# # å·¥å…·ï¼šçŸ©å½¢åŒºåŸŸæ©ç 
# # -----------------------------
# def generate_mask(H, W, top_left, bottom_right):
#     """
#     ç”ŸæˆäºŒç»´çŸ©å½¢æ©ç ï¼ˆfloat32ï¼‰
#     Args:
#         H, W: åŸå›¾é«˜å®½
#         top_left: (r1, c1) è¡Œ/åˆ—èµ·ç‚¹ï¼ˆå«ï¼‰
#         bottom_right: (r2, c2) è¡Œ/åˆ—ç»ˆç‚¹ï¼ˆå«ï¼‰
#     Return:
#         torch.FloatTensor [H, W]ï¼ŒçŸ©å½¢å†…ä¸º1ï¼Œå¤–ä¸º0
#     """
#     r1, c1 = top_left
#     r2, c2 = bottom_right
#     mask = torch.zeros((H, W), dtype=torch.float32)
#     r2 = min(r2, H - 1)
#     c2 = min(c2, W - 1)
#     mask[r1:r2 + 1, c1:c2 + 1] = 1.0
#     return mask
#
#
# # -----------------------------
# # åŒºåŸŸç‰¹å¾æå–ï¼ˆè½»é‡ CNN + æ©ç æ± åŒ–ï¼‰
# # ä¸ä½ åŸä»£ç  RegionConvSPP çš„æ¥å£ä¸€è‡´ï¼š
# # out å½¢çŠ¶ [B, 1, C]ï¼Œå †å åå¯å¾—åˆ° [B, N, C]
# # -----------------------------
# class RegionConvSPP(nn.Module):
#     def __init__(self, in_channels=1, out_channels=64):
#         super().__init__()
#         self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1, bias=False)
#         self.bn1   = nn.BatchNorm2d(32)
#         self.conv2 = nn.Conv2d(32, out_channels, kernel_size=3, padding=1, bias=False)
#         self.bn2   = nn.BatchNorm2d(out_channels)
#         self.act   = nn.ReLU(inplace=True)
#         # ä½ ä¹Ÿå¯ä»¥åœ¨è¿™é‡ŒåŠ è½»é‡ SPP/å¤šå°ºåº¦åˆ†æ”¯ï¼›è¿™é‡Œä¿æŒç®€æ´ç¨³å®š
#
#     @torch.no_grad()
#     def _down_mask(self, mask, size):
#         # mask: [H, W] -> [1,1,h',w']
#         m = mask.unsqueeze(0).unsqueeze(0).float()
#         m = F.interpolate(m, size=size, mode="nearest")
#         return m
#
#     def forward(self, x, mask_2d):
#         """
#         x: [B, 1, H, W]
#         mask_2d: [H, W]
#         return: [B, 1, C]
#         """
#         feat = self.act(self.bn1(self.conv1(x)))         # [B,32,H,W]
#         feat = self.act(self.bn2(self.conv2(feat)))      # [B,C,H,W]
#
#         m = self._down_mask(mask_2d, feat.shape[2:])     # [1,1,H',W']
#         w = m / (m.sum(dim=(2,3), keepdim=True) + 1e-8)  # å½’ä¸€åŒ–æƒé‡
#         pooled = (feat * w).sum(dim=(2,3))               # [B,C]
#         return pooled.unsqueeze(1)                       # [B,1,C]
#
#
# # -----------------------------
# # åŠŸèƒ½å…³ç³»é‚»æ¥ï¼ˆåŒä¸€æ ·æœ¬å†…ï¼ŒåŸºäºèŠ‚ç‚¹åµŒå…¥ç›¸ä¼¼åº¦ï¼‰
# # -----------------------------
# def build_functional_adj_from_feats(
#     node_feats,          # [B, N, D]
#     hard=True,           # True: äºŒå€¼ï¼ŒFalse: è¿ç»­
#     topk=2,              # æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘ä¿ç•™çš„åŠŸèƒ½è¾¹æ•°
#     tau=0.2,             # ç›¸ä¼¼åº¦é˜ˆå€¼
#     symmetrize=True,     # æ˜¯å¦å¯¹ç§°åŒ–
#     remove_self=True,    # å»æ‰å¯¹è§’
#     detach=True,         # æ˜¯å¦é˜»æ–­æ¢¯åº¦ï¼ˆå»ºè®® Trueï¼‰
#     eps=1e-8
# ):
#     X = node_feats.detach() if detach else node_feats  # [B,N,D]
#     Xn = X / (X.norm(dim=-1, keepdim=True) + eps)
#     sim = torch.einsum("bid,bjd->bij", Xn, Xn)        # [B,N,N] ä½™å¼¦ç›¸ä¼¼
#     if remove_self:
#         eye = torch.eye(sim.size(1), device=sim.device).unsqueeze(0)
#         sim = sim * (1.0 - eye)
#
#     if hard:
#         A = (sim >= tau).float()
#         if topk is not None and topk > 0:
#             vals, idx = torch.topk(sim, k=min(topk, sim.size(1)-1), dim=-1)
#             topk_mask = torch.zeros_like(sim)
#             topk_mask.scatter_(-1, idx, 1.0)
#             A = torch.max(A, topk_mask)  # é˜ˆå€¼ âˆª top-k
#         if symmetrize:
#             A = torch.max(A, A.transpose(1, 2))
#         return A                          # [B,N,N] 0/1
#     else:
#         A = F.relu(sim)
#         if symmetrize:
#             A = 0.5 * (A + A.transpose(1, 2))
#         A = A / (A.sum(dim=-1, keepdim=True) + eps)
#         return A                          # [B,N,N] è¿ç»­æƒé‡
#
#
# # -----------------------------
# # å¤šå…³ç³»å›¾æ³¨æ„åŠ›ï¼ˆå•å±‚ï¼‰
# # -----------------------------
# def _masked_softmax(logits, mask, dim=-1):
#     logits = logits.masked_fill(~mask.bool(), float('-inf'))
#     return torch.softmax(logits, dim=dim)
#
# class MultiRelGATLayer(nn.Module):
#     """
#     å¯¹æ¯ä¸ªå…³ç³» r å’Œå¤´ mï¼Œå­¦ä¹  (W_{r,m}, a_{r,m})ï¼Œé‚»æ¥æ©ç å†… softmax å½’ä¸€å¹¶èšåˆã€‚
#     è¾“å…¥:
#       H: [B, N, Din]
#       A_dict: {rel: [B,N,N] 0/1}
#       edge_dict: å¯é€‰ {rel: [B,N,N,De]}
#     è¾“å‡º:
#       H_out: [B, N, Dout]
#     """
#     def __init__(self, in_dim, out_dim, relations,
#                  num_heads=4, edge_dim=0, dropout=0.1, alpha=0.2, concat=True):
#         super().__init__()
#         assert out_dim % num_heads == 0 if concat else True, \
#             "å½“ concat=True æ—¶ï¼Œout_dim å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"
#
#         self.in_dim   = in_dim
#         self.out_dim  = out_dim
#         self.R        = list(relations)
#         self.H        = num_heads
#         self.edim     = edge_dim
#         self.concat   = concat
#         self.head_dim = out_dim // num_heads if concat else out_dim
#
#         # æ¯å…³ç³»æ¯å¤´çº¿æ€§
#         self.W = nn.ParameterDict({
#             r: nn.Parameter(torch.Tensor(self.H, in_dim, self.head_dim)) for r in self.R
#         })
#         # æ³¨æ„åŠ›å‘é‡
#         self.a_src = nn.ParameterDict({ r: nn.Parameter(torch.Tensor(self.H, self.head_dim)) for r in self.R })
#         self.a_dst = nn.ParameterDict({ r: nn.Parameter(torch.Tensor(self.H, self.head_dim)) for r in self.R })
#         self.a_edge = None
#         if edge_dim > 0:
#             self.a_edge = nn.ParameterDict({ r: nn.Parameter(torch.Tensor(self.H, edge_dim)) for r in self.R })
#
#         self.leakyrelu = nn.LeakyReLU(alpha)
#         self.dropout   = nn.Dropout(dropout)
#         self.res_proj  = nn.Linear(in_dim, out_dim, bias=False) if (in_dim != out_dim) else nn.Identity()
#         self.ln        = nn.LayerNorm(out_dim)
#
#         self.reset_parameters()
#
#     def reset_parameters(self):
#         for r in self.R:
#             nn.init.xavier_uniform_(self.W[r])
#             nn.init.xavier_uniform_(self.a_src[r].unsqueeze(-1))
#             nn.init.xavier_uniform_(self.a_dst[r].unsqueeze(-1))
#             if self.a_edge is not None:
#                 nn.init.xavier_uniform_(self.a_edge[r].unsqueeze(-1))
#         if isinstance(self.res_proj, nn.Linear):
#             nn.init.xavier_uniform_(self.res_proj.weight)
#
#     def forward(self, H, A_dict, edge_dict=None):
#         B, N, _ = H.shape
#         per_rel = []
#
#         for r in self.R:
#             A = A_dict[r].to(H.device).float()  # [B,N,N] 0/1
#             mask = A > 0
#
#             # Wh: [B,H,N,Dh]
#             Wh = torch.einsum('bnd,hdf->bh nf', H, self.W[r]).contiguous()
#             Wh = Wh.view(B, self.H, N, self.head_dim)
#
#             e_src = torch.einsum('bhnd,hd->bhn', Wh, self.a_src[r])  # [B,H,N]
#             e_dst = torch.einsum('bhnd,hd->bhn', Wh, self.a_dst[r])  # [B,H,N]
#             e = e_src.unsqueeze(-1) + e_dst.unsqueeze(-2)            # [B,H,N,N]
#
#             if (edge_dict is not None) and (r in edge_dict) and (edge_dict[r] is not None):
#                 # è¾¹ç‰¹å¾å¯åŠ æƒ
#                 eta = edge_dict[r].to(H.device).float()              # [B,N,N,De]
#                 e_edge = torch.einsum('bijn,hn->bhij', eta, self.a_edge[r])
#                 e = e + e_edge
#
#             e = self.leakyrelu(e)
#             alpha = _masked_softmax(e, mask.unsqueeze(1).expand(B, self.H, N, N), dim=-1)
#             alpha = self.dropout(alpha)
#
#             # èšåˆ
#             out_r = torch.einsum('bhij,bhjd->bhid', alpha, Wh)  # [B,H,N,Dh]
#             per_rel.append(out_r)
#
#         # è·¨å…³ç³»å¹³å‡
#         out = torch.stack(per_rel, dim=0).sum(dim=0) / float(len(per_rel))  # [B,H,N,Dh]
#
#         # åˆå¹¶å¤´
#         out = out.permute(0, 2, 1, 3).contiguous().view(B, N, self.H * self.head_dim)  # [B,N,Dout]
#
#         # æ®‹å·® + LN
#         out = self.ln(self.res_proj(H) + self.dropout(out))
#         return out
#
#
# # -----------------------------
# # MR-GAT æ¨¡å‹ï¼ˆå•å±‚ï¼‰
# # ä¸ä½ åŸ GCN çš„ç”¨æ³•å…¼å®¹ï¼šforward(H, return_node_feats=...)
# # å†…éƒ¨è‡ªåŠ¨æ„å»º A_sp / A_sym / A_fun
# # -----------------------------
# class MR_GAT(nn.Module):
#     def __init__(self, nfeat, nhid, mat_path,
#                  use_sym=True, use_fun=True, heads=4, dropout=0.2, edge_dim=0):
#         super().__init__()
#         # è¯»å–ç©ºé—´é‚»æ¥ï¼ˆç»†å±‚ï¼‰
#         adj_np = np.load(mat_path).astype(np.float32)  # [N,N]
#         self.register_buffer('A_sp', torch.from_numpy(adj_np))
#         self.use_sym = use_sym
#         self.use_fun = use_fun
#
#         # å¯¹ç§°é‚»æ¥ï¼ˆé»˜è®¤ cheeks åœ¨ç´¢å¼• 1/2ï¼‰
#         if use_sym:
#             A_sym = np.zeros_like(adj_np, dtype=np.float32)
#             if A_sym.shape[0] >= 3:
#                 A_sym[1, 2] = 1.0
#                 A_sym[2, 1] = 1.0
#             self.register_buffer('A_sym', torch.from_numpy(A_sym))
#         else:
#             self.A_sym = None
#
#         rels = ['spatial']
#         if use_sym: rels.append('symmetry')
#         if use_fun: rels.append('functional')
#
#         self.gat = MultiRelGATLayer(
#             in_dim=nfeat, out_dim=nhid, relations=rels,
#             num_heads=heads, edge_dim=edge_dim, dropout=dropout, concat=True
#         )
#         self.readout = nn.Sequential(nn.LayerNorm(nhid), nn.Linear(nhid, 2))
#
#     def _build_relations(self, H):
#         """
#         H: [B, N, D]
#         Return: {rel: [B,N,N]}
#         """
#         B, N, _ = H.shape
#         A_dict = {}
#         A_dict['spatial'] = self.A_sp.unsqueeze(0).expand(B, -1, -1).float()
#         if self.use_sym and (self.A_sym is not None):
#             A_dict['symmetry'] = self.A_sym.unsqueeze(0).expand(B, -1, -1).float()
#         if self.use_fun:
#             A_dict['functional'] = build_functional_adj_from_feats(
#                 H, hard=True, topk=2, tau=0.2, symmetrize=True, detach=True
#             ).float()
#         return A_dict
#
#     def forward(self, H, return_node_feats=False):
#         """
#         H: [B, N, nfeat] â€”â€” ä½ çš„ weighted_region_feats
#         """
#         A_dict = self._build_relations(H)
#         H1 = self.gat(H, A_dict, edge_dict=None)  # [B,N,nhid]
#         g  = H1.mean(dim=1)                       # mean pooling
#         logits = self.readout(g)                  # [B,2]
#         if return_node_feats:
#             return logits, H1
#         return logits


# model.py
import math
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F

# =============================
    # Pooling å‡½æ•°
    # =============================
def mean_pooling(H, mask=None):
        # H: [B,N,D]
    return H.mean(dim=1)  # [B,D]

def max_pooling(H, mask=None):
    return H.max(dim=1).values  # [B,D]

def attention_pooling(H, attn_mtx):
        """
        H: [B,N,D]
        attn_mtx: [B,N,N] é‚»æ¥å†…æ³¨æ„åŠ› (é€šå¸¸æ¥è‡ª GAT çš„ alpha)
        è¿”å›: [B,D] â€”â€” graph-level attention pooling
        """
        # å¯¹èŠ‚ç‚¹çº§ç‰¹å¾åš attention èšåˆ
        out = torch.einsum('bij,bjd->bid', attn_mtx, H)  # [B,N,D]
        return out.mean(dim=1)  # [B,D]ï¼Œä¹Ÿå¯ä»¥é€‰ sum

class ConcatMLPPooling(nn.Module):
        def __init__(self, N, in_dim, hidden_dim, out_dim):
            super().__init__()
            self.mlp = nn.Sequential(
                nn.Linear(N * in_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, out_dim)
            )

        def forward(self, H, mask=None):
            B, N, D = H.shape
            flat = H.reshape(B, N * D)
            return self.mlp(flat)  # [B,out_dim]

# =============================
# å·¥å…·ï¼šçŸ©å½¢åŒºåŸŸæ©ç 
# =============================
def generate_mask(H, W, top_left, bottom_right):
    """
    ç”ŸæˆäºŒç»´çŸ©å½¢æ©ç ï¼ˆfloat32ï¼‰
    Args:
        H, W: åŸå›¾é«˜å®½
        top_left: (r1, c1) è¡Œ/åˆ—èµ·ç‚¹ï¼ˆå«ï¼‰
        bottom_right: (r2, c2) è¡Œ/åˆ—ç»ˆç‚¹ï¼ˆå«ï¼‰
    Return:
        torch.FloatTensor [H, W]ï¼ŒçŸ©å½¢å†…ä¸º1ï¼Œå¤–ä¸º0
    """
    r1, c1 = top_left
    r2, c2 = bottom_right
    mask = torch.zeros((H, W), dtype=torch.float32)
    r2 = min(r2, H - 1)
    c2 = min(c2, W - 1)
    mask[r1:r2 + 1, c1:c2 + 1] = 1.0
    return mask


# =============================
# åŒºåŸŸç‰¹å¾æå–ï¼ˆè½»é‡ CNN + æ©ç æ± åŒ–ï¼‰
# ä¸ä½ åŸä»£ç  RegionConvSPP çš„æ¥å£ä¸€è‡´ï¼š
# è¾“å‡º [B, 1, C]ï¼Œå †å åå¾—åˆ° [B, N, C]
# =============================
class RegionConvSPP(nn.Module):
    def __init__(self, in_channels=1, out_channels=64):
        super().__init__()
        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=3, padding=1, bias=False)
        self.bn1   = nn.BatchNorm2d(32)
        self.conv2 = nn.Conv2d(32, out_channels, kernel_size=3, padding=1, bias=False)
        self.bn2   = nn.BatchNorm2d(out_channels)
        self.act   = nn.ReLU(inplace=True)

    @torch.no_grad()
    def _down_mask(self, mask, size):
        # mask: [H, W] -> [1,1,h',w']
        m = mask.unsqueeze(0).unsqueeze(0).float()
        m = F.interpolate(m, size=size, mode="nearest")
        return m

    def forward(self, x, mask_2d):
        """
        x: [B, 1, H, W]
        mask_2d: [H, W]
        return: [B, 1, C]
        """
        feat = self.act(self.bn1(self.conv1(x)))         # [B,32,H,W]
        feat = self.act(self.bn2(self.conv2(feat)))      # [B,C,H,W]

        m = self._down_mask(mask_2d, feat.shape[2:])     # [1,1,H',W']
        w = m / (m.sum(dim=(2,3), keepdim=True) + 1e-8)  # å½’ä¸€åŒ–æƒé‡
        pooled = (feat * w).sum(dim=(2,3))               # [B,C]
        return pooled.unsqueeze(1)                       # [B,1,C]


# =============================
# åŠŸèƒ½å…³ç³»é‚»æ¥ï¼ˆåŒä¸€æ ·æœ¬å†…ï¼ŒåŸºäºèŠ‚ç‚¹åµŒå…¥ç›¸ä¼¼åº¦ï¼‰
# =============================
def build_functional_adj_from_feats(
    node_feats,          # [B, N, D]
    hard=True,           # True: äºŒå€¼ï¼ŒFalse: è¿ç»­
    topk=1,              # æ¯ä¸ªèŠ‚ç‚¹è‡³å°‘ä¿ç•™çš„åŠŸèƒ½è¾¹æ•°ï¼ˆ>=1 é¿å…ç©ºé‚»åŸŸï¼‰
    tau=0.2,             # ç›¸ä¼¼åº¦é˜ˆå€¼
    symmetrize=True,     # æ˜¯å¦å¯¹ç§°åŒ–
    remove_self=True,    # å»æ‰å¯¹è§’
    detach=True,         # æ˜¯å¦é˜»æ–­æ¢¯åº¦ï¼ˆå»ºè®® Trueï¼‰
    eps=1e-8
):
    X = node_feats.detach() if detach else node_feats
    Xn = X / (X.norm(dim=-1, keepdim=True) + eps)
    sim = torch.einsum("bid,bjd->bij", Xn, Xn)          # [B,N,N] ä½™å¼¦ç›¸ä¼¼
    if remove_self:
        eye = torch.eye(sim.size(1), device=sim.device).unsqueeze(0)
        sim = sim * (1.0 - eye)

    if hard:
        A = (sim >= tau).float()
        if topk is not None and topk > 0:
            k = min(topk, sim.size(1) - 1)
            vals, idx = torch.topk(sim, k=k, dim=-1)
            topk_mask = torch.zeros_like(sim)
            topk_mask.scatter_(-1, idx, 1.0)
            A = torch.max(A, topk_mask)                 # é˜ˆå€¼ âˆª top-k
        if symmetrize:
            A = torch.max(A, A.transpose(1, 2))
        return A                                        # [B,N,N] 0/1
    else:
        A = F.relu(sim)
        if symmetrize:
            A = 0.5 * (A + A.transpose(1, 2))
        A = A / (A.sum(dim=-1, keepdim=True) + eps)
        return A                                        # [B,N,N] è¿ç»­æƒé‡


# =============================
# å®‰å…¨ masked softmaxï¼šé˜²ç©ºé‚»åŸŸ / é˜² NaN
# =============================
def _masked_softmax(logits, mask, dim=-1):
    """
    åœ¨é‚»æ¥æ©ç å†…åš softmaxï¼›è‹¥æŸè¡Œé‚»åŸŸå…¨ç©ºï¼Œåˆ™è¯¥è¡Œæƒé‡ç½® 0ï¼ˆé¿å… NaNï¼‰ã€‚
    """
    mask = mask.bool()
    masked = logits.masked_fill(~mask, float('-inf'))

    # æ‰¾å‡ºâ€œæ•´è¡Œéƒ½è¢«å±è”½â€çš„æƒ…å†µ
    all_false = (~mask).all(dim=dim, keepdim=True)

    # é¿å… softmax(-inf...-inf) -> NaN
    masked = masked.masked_fill(all_false, 0.0)
    out = torch.softmax(masked, dim=dim)

    # çœŸçš„ç©ºé‚»åŸŸè¡Œï¼Œç›´æ¥å…¨ 0
    out = torch.where(all_false, torch.zeros_like(out), out)
    return out


# =============================
# å¤šå…³ç³»å›¾æ³¨æ„åŠ›ï¼ˆå•å±‚ï¼‰
# =============================
class MultiRelGATLayer(nn.Module):
    """
    å¯¹æ¯ä¸ªå…³ç³» r å’Œå¤´ mï¼Œå­¦ä¹  (W_{r,m}, a_{r,m})ï¼Œé‚»æ¥æ©ç å†… softmax å½’ä¸€å¹¶èšåˆã€‚
    è¾“å…¥:
      H: [B, N, Din]
      A_dict: {rel: [B,N,N] 0/1}
      edge_dict: å¯é€‰ {rel: [B,N,N,De]}
    è¾“å‡º:
      H_out: [B, N, Dout]
    """
    def __init__(self, in_dim, out_dim, relations,
                 num_heads=4, edge_dim=0, dropout=0.1, alpha=0.2, concat=True):
        super().__init__()
        assert (not concat) or (out_dim % num_heads == 0), \
            "å½“ concat=True æ—¶ï¼Œout_dim å¿…é¡»èƒ½è¢« num_heads æ•´é™¤"

        self.in_dim   = in_dim
        self.out_dim  = out_dim
        self.R        = list(relations)
        self.H        = num_heads
        self.edim     = edge_dim
        self.concat   = concat
        self.head_dim = out_dim // num_heads if concat else out_dim

        # æ¯å…³ç³»æ¯å¤´çº¿æ€§
        self.W = nn.ParameterDict({
            r: nn.Parameter(torch.Tensor(self.H, in_dim, self.head_dim)) for r in self.R
        })
        # æ³¨æ„åŠ›å‘é‡
        self.a_src = nn.ParameterDict({ r: nn.Parameter(torch.Tensor(self.H, self.head_dim)) for r in self.R })
        self.a_dst = nn.ParameterDict({ r: nn.Parameter(torch.Tensor(self.H, self.head_dim)) for r in self.R })
        self.a_edge = None
        if edge_dim > 0:
            self.a_edge = nn.ParameterDict({ r: nn.Parameter(torch.Tensor(self.H, edge_dim)) for r in self.R })

        self.leakyrelu = nn.LeakyReLU(alpha)
        self.dropout   = nn.Dropout(dropout)
        self.res_proj  = nn.Linear(in_dim, out_dim, bias=False) if (in_dim != out_dim) else nn.Identity()
        self.ln        = nn.LayerNorm(out_dim)
        self.reset_parameters()

    def reset_parameters(self):
        for r in self.R:
            nn.init.xavier_uniform_(self.W[r])
            nn.init.xavier_uniform_(self.a_src[r].unsqueeze(-1))
            nn.init.xavier_uniform_(self.a_dst[r].unsqueeze(-1))
            if self.a_edge is not None:
                nn.init.xavier_uniform_(self.a_edge[r].unsqueeze(-1))
        if isinstance(self.res_proj, nn.Linear):
            nn.init.xavier_uniform_(self.res_proj.weight)

    def forward(self, H, A_dict, edge_dict=None):
        """
        H: [B, N, Din]
        A_dict: {rel: [B,N,N]}
        edge_dict: å¯é€‰ {rel: [B,N,N,De]}
        """
        B, N, _ = H.shape
        per_rel = []

        for r in self.R:
            A = A_dict[r].to(H.device).float()              # [B,N,N] 0/1
            mask = A > 0                                    # [B,N,N] bool

            # Wh: [B,H,N,Dh]
            # ä¿®æ­£ einsumï¼ˆé¿å… 'bh nf' çš„ç©ºæ ¼é—®é¢˜ï¼‰
            # çº¿æ€§æŠ•å½±ï¼šWh [B, H, N, Dh]
            Wh = torch.einsum('bnd,hdf->bhnf', H, self.W[r])  # æ³¨æ„è¿™é‡Œæ˜¯ bhnf

            # æ³¨æ„åŠ›æ‰“åˆ†ï¼ˆæº/ç›®çš„ï¼‰ï¼Œa_src/a_dst å½¢çŠ¶ä¸º [H, Dh]
            e_src = torch.einsum('bhnf,hf->bhn', Wh, self.a_src[r])  # [B,H,N]
            e_dst = torch.einsum('bhnf,hf->bhn', Wh, self.a_dst[r])  # [B,H,N]
            e = e_src.unsqueeze(-1) + e_dst.unsqueeze(-2)  # [B,H,N,N]

            # ï¼ˆå¯é€‰ï¼‰è¾¹ç‰¹å¾ï¼šeta [B,N,N,De]ï¼Œa_edge[r] [H,De]
            if (edge_dict is not None) and (r in edge_dict) and (edge_dict[r] is not None):
                eta = edge_dict[r].to(H.device).float()  # [B,N,N,De]
                e_edge = torch.einsum('bijd,hd->bhij', eta, self.a_edge[r])
                e = e + e_edge

            e = self.leakyrelu(e)

            # é‚»åŸŸå†… softmaxï¼ˆç”¨ä½ çš„ _masked_softmax é˜²ç©ºé‚»åŸŸï¼‰
            alpha = _masked_softmax(e, mask.unsqueeze(1).expand(B, self.H, N, N), dim=-1)
            alpha = torch.nan_to_num(alpha, nan=0.0, posinf=0.0, neginf=0.0)

            # èšåˆï¼šout_r [B,H,N,Dh]
            out_r = torch.einsum('bhij,bhjn->bhin', alpha, Wh)
            per_rel.append(out_r)

        # è·¨å…³ç³»æ±‚å’Œ / å¹³å‡
        out = torch.stack(per_rel, dim=0).sum(dim=0) / float(len(per_rel))  # [B,H,N,Dh]

        # åˆå¹¶å¤šå¤´
        out = out.permute(0, 2, 1, 3).contiguous()  # [B,N,H,Dh]
        out = out.view(B, N, self.H * self.head_dim)  # [B,N,Dout]

        # æ®‹å·® + LN
        out = self.ln(self.res_proj(H) + self.dropout(out))
        return out


# =============================
# è‡ªç¯å·¥å…·
# =============================
def _add_self_loops(A):  # A: [B, N, N]
    B, N, _ = A.shape
    I = torch.eye(N, device=A.device).unsqueeze(0).expand(B, -1, -1)
    return torch.clamp(A + I, max=1.0)


# =============================
# MR-GAT æ¨¡å‹ï¼ˆå•å±‚ï¼‰
# ä¸åŸ GCN ç”¨æ³•å…¼å®¹ï¼šforward(H, return_node_feats=...)
# å†…éƒ¨è‡ªåŠ¨æ„å»º A_sp / A_sym / A_funï¼ˆå¯é€‰ï¼‰
# =============================
class MR_GAT(nn.Module):
    def __init__(self, nfeat, nhid, mat_path, use_fun=True, heads=4, dropout=0.2, edge_dim=0, pooling='mean'):
        super().__init__()

        # è¯»å–ç©ºé—´é‚»æ¥ï¼ˆç¨€ç–/äºŒå€¼ï¼‰
        adj_np = np.load(mat_path).astype(np.float32)  # [N,N]
        self.register_buffer('A_sp', torch.from_numpy(adj_np))
        self.use_fun = use_fun

        rels = ['spatial']
        if use_fun: rels.append('functional')

        self.gat = MultiRelGATLayer(
            in_dim=nfeat, out_dim=nhid, relations=rels,
            num_heads=heads, edge_dim=edge_dim, dropout=dropout, concat=True
        )

        # ğŸ”¥ é€‰æ‹© pooling
        self.pooling = pooling.lower()
        if self.pooling == "concat+mlp":
            # æ³¨æ„ï¼šN è¦æå‰çŸ¥é“ï¼ˆè¿™é‡Œä» mat_path è¯»åˆ°çš„é‚»æ¥çŸ©é˜µå¤§å°ï¼‰
            N = self.A_sp.shape[0]
            self.concat_pooler = ConcatMLPPooling(N, nhid, hidden_dim=128, out_dim=nhid)

        self.readout = nn.Sequential(nn.LayerNorm(nhid), nn.Linear(nhid, 2))

    def _build_relations(self, H):
        """
        H: [B, N, D]
        Return: {rel: [B,N,N]}ï¼Œç»Ÿä¸€åŠ è‡ªç¯ï¼Œé¿å…ç©ºé‚»åŸŸ
        """
        B, N, _ = H.shape
        A_dict = {}
        A_dict['spatial'] = self.A_sp.unsqueeze(0).expand(B, -1, -1).float()
        if self.use_fun:
            A_dict['functional'] = build_functional_adj_from_feats(
                H, hard=True, topk=1, tau=0.2, symmetrize=True, detach=True
            ).float()

        # ç»Ÿä¸€åŠ è‡ªç¯ï¼Œé˜²æ­¢ç©ºé‚»åŸŸå¯¼è‡´ softmax NaN
        for k in list(A_dict.keys()):
            A_dict[k] = _add_self_loops(A_dict[k])

        return A_dict

    def forward(self, H, return_node_feats=False):
        """
        H: [B, N, nfeat] â€”â€” ä½ çš„ weighted_region_feats
        """
        A_dict = self._build_relations(H)
        H1 = self.gat(H, A_dict, edge_dict=None)  # [B,N,nhid]

        if self.pooling == "mean":
            g = mean_pooling(H1)
        elif self.pooling == "max":
            g = max_pooling(H1)
        elif self.pooling == "attention":
            # ç”¨åŠŸèƒ½é‚»æ¥ A_dict['functional'] æˆ–èåˆåçš„é‚»æ¥æ¥åš attn pooling
            g = attention_pooling(H1, A_dict['functional'])
        elif self.pooling == "concat+mlp":
            g = self.concat_pooler(H1)

        else:
            raise ValueError(f"Unknown pooling type: {self.pooling}")

        logits = self.readout(g)                  # [B,2]
        if return_node_feats:
            return logits, H1
        return logits
